#!/bin/bash -l
#SBATCH --job-name=train_cxr_clip
#SBATCH --ntasks=1
#SBATCH --gres=gpu:a100:8 -C a100_80
#SBATCH --partition=a100
#SBATCH --output=R-%x.%j.out
#SBATCH --error=R-%x.%j.err
#SBATCH --mail-type=end,fail
#SBATCH --time=24:00:00
#SBATCH --export=NONE

unset SLURM_EXPORT_ENV

cd $WORK/cxr_phrase_grounding || exit

mkdir $TMPDIR/.cache
mkdir $TMPDIR/.cache/hub

export PYTHONPATH=$PWD
export HF_HOME=$TMPDIR/.cache
export HF_HUB_CACHE=$TMPDIR/.cache/hub
export WANDB_CACHE_DIR=$TMPDIR/.cache/

chmod +x training/launch_scripts/launch_training_cxrclip_hpc


# Set proxy to access internet from the node
export http_proxy=http://proxy:80
export https_proxy=http://proxy:80
export TORCH_NCCL_ASYNC_ERROR_HANDLING=1

module purge
module load python/3.9-anaconda
module load cudnn/9.2.0
module load cuda/11.8

# Conda
source activate cxr_phrase_grounding

# Run training script (with data copied to node)
srun training/launch_scripts/launch_training_cxrclip_hpc

if [[ $? -eq 124 ]]; then
  sbatch slurm_train_cxrclip
fi
